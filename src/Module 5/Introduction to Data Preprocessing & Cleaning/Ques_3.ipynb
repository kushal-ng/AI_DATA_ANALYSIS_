{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps in Data Preprocessing\n",
    "\n",
    "# 1. Data Collection: Gathering raw data from various sources.\n",
    "# Task 1: Collect data from two different sources and merge them.\n",
    "# Task 2: Validate the integrity of the collected datasets.\n",
    "# Task 3: Reflect on challenges faced during data collection and how they were addressed.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 1: Collect data from two different sources and merge them\n",
    "# For demonstration, create two sample DataFrames mimicking data sources\n",
    "\n",
    "data_source1 = {\n",
    "    'id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35]\n",
    "}\n",
    "\n",
    "data_source2 = {\n",
    "    'id': [3, 4, 5],\n",
    "    'name': ['Charlie', 'David', 'Eve'],\n",
    "    'age': [35, 40, 28]\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data_source1)\n",
    "df2 = pd.DataFrame(data_source2)\n",
    "\n",
    "# Merge datasets on 'id', using outer join to keep all records\n",
    "merged_df = pd.merge(df1, df2, on=['id', 'name', 'age'], how='outer')\n",
    "\n",
    "print(\"Merged DataFrame:\")\n",
    "print(merged_df)\n",
    "\n",
    "# Task 2: Validate integrity of the merged dataset\n",
    "# Check for duplicates\n",
    "duplicates = merged_df.duplicated()\n",
    "print(\"\\nDuplicate rows in merged dataset:\")\n",
    "print(merged_df[duplicates])\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = merged_df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Task 3: Reflection (as comment)\n",
    "# Challenges faced during data collection:\n",
    "# - Data format inconsistencies between sources\n",
    "# - Overlapping records requiring careful merging to avoid duplicates\n",
    "# - Missing or inconsistent data fields\n",
    "# Addressed by:\n",
    "# - Standardizing column names and formats before merging\n",
    "# - Using outer joins to avoid loss of records\n",
    "# - Validating merged data for duplicates and missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Cleaning: Addressing missing values, duplicates, incorrect types, and outliers.\n",
    "# Task 1: Clean a given dataset and document the changes made.\n",
    "# Task 2: Create a checklist to ensure comprehensive data cleaning in future projects.\n",
    "# Task 3: Collaborate with a peer to clean a new dataset and present your solutions.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'id': [1, 2, 2, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Bob', 'David', None],\n",
    "    'age': ['25', 'thirty', '30', 40, 35],\n",
    "    'salary': [50000, 60000, 60000, 800000, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "df_cleaned['age'] = pd.to_numeric(df_cleaned['age'], errors='coerce')\n",
    "\n",
    "age_median = df_cleaned['age'].median()\n",
    "df_cleaned['age'].fillna(age_median, inplace=True)\n",
    "\n",
    "salary_median = df_cleaned['salary'].median()\n",
    "df_cleaned['salary'].fillna(salary_median, inplace=True)\n",
    "\n",
    "q1 = df_cleaned['salary'].quantile(0.25)\n",
    "q3 = df_cleaned['salary'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "df_cleaned.loc[df_cleaned['salary'] > upper_bound, 'salary'] = salary_median\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nCleaned DataFrame:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Transformation: Modifying data to fit specific analytical requirements.\n",
    "# Task 1: Transform a date column into separate 'day', 'month', and 'year' columns.\n",
    "# Task 2: Apply normalization to a dataset feature and confirm the changes.\n",
    "# Task 3: Discuss the importance of data transformation in model interpretability.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = {\n",
    "    'date': ['2023-01-15', '2023-03-22', '2023-07-30'],\n",
    "    'feature': [10, 15, 20]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df['feature_normalized'] = scaler.fit_transform(df[['feature']])\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Scaling: Adjusting data features to a common scale.\n",
    "# Task 1: Apply Min-Max scaling to a dataset.\n",
    "# Task 2: Standardize a dataset and visualize the changes with a histogram.\n",
    "# Task 3: Analyze how feature scaling impacts the performance of different machine learning algorithms.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = {\n",
    "    'feature1': [10, 20, 30, 40, 50],\n",
    "    'feature2': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_minmax_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_minmax_scaled['feature1'], bins=5, alpha=0.7)\n",
    "plt.title('Min-Max Scaled feature1')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(df_standard_scaled['feature1'], bins=5, alpha=0.7)\n",
    "plt.title('Standard Scaled feature1')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering: Creating new features from existing ones to improve model accuracy.\n",
    "# Task 1: Create a new synthetic feature from existing dataset features.\n",
    "# Task 2: Evaluate the impact of new features on model accuracy.\n",
    "# Task 3: Read an academic paper on feature engineering techniques and present the findings.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = {\n",
    "    'age': [25, 45, 35, 50, 23, 40, 60, 30],\n",
    "    'salary': [50000, 80000, 60000, 90000, 48000, 75000, 100000, 52000],\n",
    "    'purchased': [0, 1, 0, 1, 0, 1, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['age_salary_ratio'] = df['age'] / df['salary']\n",
    "\n",
    "X = df[['age', 'salary']]\n",
    "X_new = df[['age', 'salary', 'age_salary_ratio']]\n",
    "y = df['purchased']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "X_new_train, X_new_test, y_train, y_test = train_test_split(X_new, y, random_state=42)\n",
    "\n",
    "model_original = LogisticRegression()\n",
    "model_original.fit(X_train, y_train)\n",
    "y_pred_original = model_original.predict(X_test)\n",
    "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
    "\n",
    "model_new = LogisticRegression()\n",
    "model_new.fit(X_new_train, y_train)\n",
    "y_pred_new = model_new.predict(X_new_test)\n",
    "accuracy_new = accuracy_score(y_test, y_pred_new)\n",
    "\n",
    "print(f'Accuracy without new feature: {accuracy_original:.2f}')\n",
    "print(f'Accuracy with new feature: {accuracy_new:.2f}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
