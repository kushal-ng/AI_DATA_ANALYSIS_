{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Load a CSV Dataset\n",
    "# Description: Load a CSV file into a Pandas DataFrame and print the first five rows to understand the structure of the dataset.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 1: Load a CSV Dataset\n",
    "# Description: Load a CSV file into a Pandas DataFrame and print the first five rows.\n",
    "\n",
    "def load_and_preview_csv(file_path):\n",
    "    # Load CSV into DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Print first five rows\n",
    "    print(\"First 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace 'your_dataset.csv' with your actual CSV file path\n",
    "    df = load_and_preview_csv('your_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Check for Missing Values\n",
    "# Description: Identify and list the columns with missing values and the number of missing values in each.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 2: Check for Missing Values\n",
    "# Description: Identify and list columns with missing values and the count of missing values in each.\n",
    "\n",
    "def check_missing_values(df):\n",
    "    missing_counts = df.isnull().sum()\n",
    "    missing_columns = missing_counts[missing_counts > 0]\n",
    "    \n",
    "    if missing_columns.empty:\n",
    "        print(\"No missing values found in any column.\")\n",
    "    else:\n",
    "        print(\"Columns with missing values and their counts:\")\n",
    "        print(missing_columns)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assume df is already loaded from previous task or elsewhere\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with actual path\n",
    "    check_missing_values(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Visualize Missing Data\n",
    "# Description: Use a heatmap to visualize the missing values in the dataset.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task 3: Visualize Missing Data\n",
    "# Description: Use a heatmap to visualize the missing values in the dataset.\n",
    "\n",
    "def visualize_missing_data(df):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "    plt.title('Missing Data Heatmap')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    visualize_missing_data(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Remove Columns with Many Missing Values\n",
    "# Description: Drop columns that have more than 50% missing values.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 4: Remove Columns with Many Missing Values\n",
    "# Description: Drop columns that have more than 50% missing values.\n",
    "\n",
    "def drop_columns_with_many_missing(df, threshold=0.5):\n",
    "    # Calculate the fraction of missing values per column\n",
    "    missing_fraction = df.isnull().mean()\n",
    "    # Identify columns to drop\n",
    "    cols_to_drop = missing_fraction[missing_fraction > threshold].index\n",
    "    # Drop columns\n",
    "    df_cleaned = df.drop(columns=cols_to_drop)\n",
    "    return df_cleaned\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    df_cleaned = drop_columns_with_many_missing(df)\n",
    "    print(\"Columns dropped:\", df.columns.difference(df_cleaned.columns).tolist())\n",
    "    print(df_cleaned.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Identify Duplicate Rows\n",
    "# Description: Check for and display any duplicate rows in the dataset.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 5: Identify Duplicate Rows\n",
    "# Description: Check for and display any duplicate rows in the dataset.\n",
    "\n",
    "def find_duplicate_rows(df):\n",
    "    # Find duplicate rows (excluding the first occurrence)\n",
    "    duplicates = df[df.duplicated(keep='first')]\n",
    "    return duplicates\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    duplicate_rows = find_duplicate_rows(df)\n",
    "    if not duplicate_rows.empty:\n",
    "        print(\"Duplicate rows found:\")\n",
    "        print(duplicate_rows)\n",
    "    else:\n",
    "        print(\"No duplicate rows found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6: Remove Duplicate Rows\n",
    "# Description: Remove duplicate rows from the dataset and verify that they have been removed.\n",
    "import pandas as pd\n",
    "\n",
    "# Task 6: Remove Duplicate Rows\n",
    "# Description: Remove duplicate rows from the dataset and verify that they have been removed.\n",
    "\n",
    "def remove_duplicate_rows(df):\n",
    "    # Remove duplicate rows, keep the first occurrence\n",
    "    df_no_duplicates = df.drop_duplicates(keep='first')\n",
    "    return df_no_duplicates\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    \n",
    "    df_cleaned = remove_duplicate_rows(df)\n",
    "    print(f\"Dataset shape after removing duplicates: {df_cleaned.shape}\")\n",
    "    \n",
    "    # Verify no duplicates remain\n",
    "    duplicates_after = df_cleaned[df_cleaned.duplicated()]\n",
    "    if duplicates_after.empty:\n",
    "        print(\"All duplicate rows removed successfully.\")\n",
    "    else:\n",
    "        print(\"Duplicates still exist in the dataset.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 7: Check Data Inconsistencies\n",
    "# Description: Identify inconsistencies in categorical columns, such as differing text cases or trailing spaces.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 7: Check Data Inconsistencies\n",
    "# Description: Identify inconsistencies in categorical columns, such as differing text cases or trailing spaces.\n",
    "\n",
    "def check_categorical_inconsistencies(df, categorical_columns):\n",
    "    inconsistencies = {}\n",
    "    for col in categorical_columns:\n",
    "        if col in df.columns:\n",
    "            unique_vals = df[col].dropna().astype(str).unique()\n",
    "            # Check for case differences by comparing lowercase versions\n",
    "            lower_case_vals = set(val.lower().strip() for val in unique_vals)\n",
    "            if len(lower_case_vals) != len(unique_vals):\n",
    "                inconsistencies[col] = {\n",
    "                    \"original_unique_values\": unique_vals,\n",
    "                    \"note\": \"Inconsistencies detected (case or whitespace differences).\"\n",
    "                }\n",
    "    return inconsistencies\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    categorical_cols = ['Category', 'Status', 'Type']  # Replace with your categorical columns\n",
    "    \n",
    "    issues = check_categorical_inconsistencies(df, categorical_cols)\n",
    "    if issues:\n",
    "        print(\"Inconsistencies found in categorical columns:\")\n",
    "        for col, detail in issues.items():\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"Unique Values: {detail['original_unique_values']}\")\n",
    "            print(detail[\"note\"])\n",
    "    else:\n",
    "        print(\"No inconsistencies found in specified categorical columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8: Get Summary of Data Quality\n",
    "# Description: Generate a summary of data quality including total records, number of duplicate rows, and columns with missing values.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 8: Get Summary of Data Quality\n",
    "# Description: Generate a summary of data quality including total records, number of duplicate rows, and columns with missing values.\n",
    "\n",
    "def data_quality_summary(df):\n",
    "    total_records = len(df)\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    missing_values = df.isnull().sum()\n",
    "    columns_with_missing = missing_values[missing_values > 0].to_dict()\n",
    "    \n",
    "    summary = {\n",
    "        \"Total Records\": total_records,\n",
    "        \"Duplicate Rows\": duplicate_count,\n",
    "        \"Columns with Missing Values\": columns_with_missing\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "    \n",
    "    summary = data_quality_summary(df)\n",
    "    print(\"Data Quality Summary:\")\n",
    "    for key, value in summary.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9: Generate a Data Quality Report\n",
    "# Description: Create a comprehensive data quality report that includes not only missing values but also basic statistics for numerical columns and the distribution of categorical columns.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 9: Generate a Data Quality Report\n",
    "# Description: Create a comprehensive data quality report that includes missing values,\n",
    "# basic statistics for numerical columns, and distribution of categorical columns.\n",
    "\n",
    "def generate_data_quality_report(df):\n",
    "    report = {}\n",
    "\n",
    "    # Missing values per column\n",
    "    missing_values = df.isnull().sum()\n",
    "    report['Missing Values'] = missing_values[missing_values > 0].to_dict()\n",
    "\n",
    "    # Basic statistics for numerical columns\n",
    "    numeric_stats = df.describe().to_dict()\n",
    "    report['Numerical Summary Statistics'] = numeric_stats\n",
    "\n",
    "    # Distribution of categorical columns\n",
    "    categorical_distributions = {}\n",
    "    for col in df.select_dtypes(include=['object', 'category']).columns:\n",
    "        value_counts = df[col].value_counts(dropna=False).to_dict()\n",
    "        categorical_distributions[col] = value_counts\n",
    "    report['Categorical Distributions'] = categorical_distributions\n",
    "\n",
    "    return report\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "\n",
    "    dq_report = generate_data_quality_report(df)\n",
    "    \n",
    "    print(\"Data Quality Report:\\n\")\n",
    "    \n",
    "    print(\"Missing Values:\")\n",
    "    for col, count in dq_report['Missing Values'].items():\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "    print(\"\\nNumerical Summary Statistics:\")\n",
    "    for stat, values in dq_report['Numerical Summary Statistics'].items():\n",
    "        print(f\"  {stat}:\")\n",
    "        for col, val in values.items():\n",
    "            print(f\"    {col}: {val}\")\n",
    "\n",
    "    print(\"\\nCategorical Distributions:\")\n",
    "    for col, dist in dq_report['Categorical Distributions'].items():\n",
    "        print(f\"  {col}:\")\n",
    "        for val, count in dist.items():\n",
    "            print(f\"    {val}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 10: Advanced Data Imputation\n",
    "# Description: Perform advanced data imputation by replacing missing values in numerical columns with the mean and categorical columns with the mode.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Task 10: Advanced Data Imputation\n",
    "# Description: Replace missing values in numerical columns with mean and in categorical columns with mode.\n",
    "\n",
    "def advanced_data_imputation(df):\n",
    "    df_imputed = df.copy()\n",
    "\n",
    "    # Impute numerical columns with mean\n",
    "    num_cols = df_imputed.select_dtypes(include=['number']).columns\n",
    "    for col in num_cols:\n",
    "        mean_value = df_imputed[col].mean()\n",
    "        df_imputed[col].fillna(mean_value, inplace=True)\n",
    "\n",
    "    # Impute categorical columns with mode\n",
    "    cat_cols = df_imputed.select_dtypes(include=['object', 'category']).columns\n",
    "    for col in cat_cols:\n",
    "        mode_value = df_imputed[col].mode()\n",
    "        if not mode_value.empty:\n",
    "            df_imputed[col].fillna(mode_value[0], inplace=True)\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('your_dataset.csv')  # Replace with your dataset path\n",
    "\n",
    "    df_imputed = advanced_data_imputation(df)\n",
    "    print(df_imputed.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
