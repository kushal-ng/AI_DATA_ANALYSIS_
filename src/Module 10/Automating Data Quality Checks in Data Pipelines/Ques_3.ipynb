{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Data Quality Monitoring\n",
    "**Objective**: Use Great Expectations to perform data profiling and write validation rules.\n",
    "\n",
    "1. Data Profiling with Great Expectations\n",
    "\n",
    "### Profile a JSON dataset with product sales data to check for null values in the 'ProductID' and 'Price' fields.\n",
    "- Create an expectation suite and connect it to the data context.\n",
    "- Use the `expect_column_values_to_not_be_null` expectation to profile these fields.\n",
    "- Review the summary to identify any unexpected null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import great_expectations as ge\n",
    "import os\n",
    "\n",
    "project_root_dir = os.getcwd()\n",
    "context = ge.get_context(context_root_dir=project_root_dir)\n",
    "\n",
    "# Load JSON data batch for profiling\n",
    "batch = context.get_batch({\n",
    "    \"datasource_name\": \"sales_data_source\",\n",
    "    \"data_connector_name\": \"default_runtime_data_connector_name\",\n",
    "    \"data_asset_name\": \"product_sales_json\",\n",
    "    \"runtime_parameters\": {\"path\": \"data/product_sales.json\"},\n",
    "    \"batch_identifiers\": {\"default_identifier_name\": \"profile_run\"},\n",
    "})\n",
    "\n",
    "suite_name = \"product_sales_profile_suite\"\n",
    "try:\n",
    "    context.create_expectation_suite(expectation_suite_name=suite_name, overwrite_existing=True)\n",
    "except Exception as e:\n",
    "    print(f\"Suite creation or overwrite error: {e}\")\n",
    "\n",
    "batch.expect_column_values_to_not_be_null(column=\"ProductID\")\n",
    "batch.expect_column_values_to_not_be_null(column=\"Price\")\n",
    "\n",
    "context.save_expectation_suite(batch.get_expectation_suite(), suite_name)\n",
    "\n",
    "results = context.run_validation_operator(\n",
    "    \"action_list_operator\",\n",
    "    assets_to_validate=[batch],\n",
    "    run_name=\"profile_validation_run\"\n",
    ")\n",
    "\n",
    "validation_result = results.list_validation_results()[0]\n",
    "print(f\"Validation Success: {validation_result.success}\")\n",
    "for result in validation_result.results:\n",
    "    if result.expectation_config.expectation_type == \"expect_column_values_to_not_be_null\":\n",
    "        print(f\"Column: {result.expectation_config.kwargs['column']}\")\n",
    "        print(f\"Success: {result.success}\")\n",
    "        print(f\"Unexpected Nulls Count: {result.result.get('unexpected_count', 0)}\")\n",
    "\n",
    "context.build_data_docs()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Writing Validation Rules for Data Ingestion\n",
    "\n",
    "### Define validation rules for an API data source to confirm that 'Status' field contains only predefined statuses ('Active', 'Inactive').\n",
    "\n",
    "- Apply `expect_column_values_to_be_in_set` to check field values during data ingestion.\n",
    "- Execute the validation and review any mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import great_expectations as ge\n",
    "import os\n",
    "\n",
    "project_root_dir = os.getcwd()\n",
    "context = ge.get_context(context_root_dir=project_root_dir)\n",
    "\n",
    "# Load API data batch for validation (assume JSON file from API response saved locally for example)\n",
    "batch = context.get_batch({\n",
    "    \"datasource_name\": \"api_data_source\",\n",
    "    \"data_connector_name\": \"default_runtime_data_connector_name\",\n",
    "    \"data_asset_name\": \"api_status_data\",\n",
    "    \"runtime_parameters\": {\"path\": \"data/api_status.json\"},\n",
    "    \"batch_identifiers\": {\"default_identifier_name\": \"validation_run\"},\n",
    "})\n",
    "\n",
    "suite_name = \"api_status_validation_suite\"\n",
    "try:\n",
    "    context.create_expectation_suite(expectation_suite_name=suite_name, overwrite_existing=True)\n",
    "except Exception as e:\n",
    "    print(f\"Suite creation or overwrite error: {e}\")\n",
    "\n",
    "allowed_statuses = [\"Active\", \"Inactive\"]\n",
    "batch.expect_column_values_to_be_in_set(column=\"Status\", value_set=allowed_statuses)\n",
    "\n",
    "context.save_expectation_suite(batch.get_expectation_suite(), suite_name)\n",
    "\n",
    "results = context.run_validation_operator(\n",
    "    \"action_list_operator\",\n",
    "    assets_to_validate=[batch],\n",
    "    run_name=\"api_status_validation_run\"\n",
    ")\n",
    "\n",
    "validation_result = results.list_validation_results()[0]\n",
    "print(f\"Validation Success: {validation_result.success}\")\n",
    "for result in validation_result.results:\n",
    "    if result.expectation_config.expectation_type == \"expect_column_values_to_be_in_set\":\n",
    "        print(f\"Column: {result.expectation_config.kwargs['column']}\")\n",
    "        print(f\"Success: {result.success}\")\n",
    "        print(f\"Unexpected Count: {result.result.get('unexpected_count', 0)}\")\n",
    "        if not result.success:\n",
    "            print(f\"Unexpected Values Sample: {result.result.get('partial_unexpected_list', [])}\")\n",
    "\n",
    "context.build_data_docs()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
