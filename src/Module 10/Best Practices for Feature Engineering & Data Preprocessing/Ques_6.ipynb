{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Large-scale ML Pipelines:\n",
    "\n",
    "**Task 1**: Impute with Mean or Median\n",
    "- Step 1: Load a dataset with missing values (e.g., Boston Housing dataset).\n",
    "- Step 2: Identify columns with missing values.\n",
    "- Step 3: Impute missing values using the mean or median of the respective columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def impute_missing_values(strategy='mean'):\n",
    "    try:\n",
    "        boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
    "        df = boston.frame\n",
    "\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            # artificially create missing values for demonstration\n",
    "            df.iloc[0, 0] = np.nan\n",
    "            df.iloc[4, 3] = np.nan\n",
    "\n",
    "        imputer = SimpleImputer(strategy=strategy)\n",
    "        df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "\n",
    "        return df_imputed\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Impute with the Most Frequent Value\n",
    "- Step 1: Use the Titanic dataset and identify columns with missing values.\n",
    "- Step 2: Impute categorical columns using the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "def impute_most_frequent():\n",
    "    try:\n",
    "        titanic_url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "        df = pd.read_csv(titanic_url)\n",
    "\n",
    "        cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "        df_cat = df[cat_cols]\n",
    "\n",
    "        imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_cat_imputed = pd.DataFrame(imputer.fit_transform(df_cat), columns=cat_cols)\n",
    "\n",
    "        for col in cat_cols:\n",
    "            df[col] = df_cat_imputed[col]\n",
    "\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Advanced Imputation - k-Nearest Neighbors\n",
    "- Step 1: Implement KNN imputation using the KNNImputer from sklearn.\n",
    "- Step 2: Explore how KNN imputation improves data completion over simpler methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def knn_imputation(data, n_neighbors=5):\n",
    "    try:\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            return \"Input data must be a pandas DataFrame.\"\n",
    "\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "\n",
    "        data_imputed_array = imputer.fit_transform(data[numeric_cols])\n",
    "        data_imputed = data.copy()\n",
    "        data_imputed[numeric_cols] = data_imputed_array\n",
    "\n",
    "        return data_imputed\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling & Normalization Best Practices:\n",
    "\n",
    "**Task 1**: Standardization\n",
    "- Step 1: Standardize features using StandardScaler.\n",
    "- Step 2: Observe how standardization affects data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "def standardize_features(data):\n",
    "    try:\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            return \"Input must be a pandas DataFrame.\"\n",
    "        numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "        scaler = StandardScaler()\n",
    "        data_scaled = data.copy()\n",
    "        data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "        return data_scaled\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Min-Max Scaling\n",
    "\n",
    "- Step 1: Scale features to lie between 0 and 1 using MinMaxScaler.\n",
    "- Step 2: Compare with standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "def min_max_scale_features(data):\n",
    "    try:\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            return \"Input must be a pandas DataFrame.\"\n",
    "        numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "        scaler = MinMaxScaler()\n",
    "        data_scaled = data.copy()\n",
    "        data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "        return data_scaled\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Robust Scaling\n",
    "- Step 1: Scale features using RobustScaler, which is useful for data with outliers.\n",
    "- Step 2: Assess changes in data scaling compared to other scaling methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "\n",
    "def robust_scale_features(data):\n",
    "    try:\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            return \"Input must be a pandas DataFrame.\"\n",
    "        numeric_cols = data.select_dtypes(include=['number']).columns\n",
    "        scaler = RobustScaler()\n",
    "        data_scaled = data.copy()\n",
    "        data_scaled[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "        return data_scaled\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection Techniques:\n",
    "### Removing Highly Correlated Features:\n",
    "\n",
    "**Task 1**: Correlation Matrix\n",
    "- Step 1: Compute correlation matrix.\n",
    "- Step 2: Remove highly correlated features (correlation > 0.9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def remove_highly_correlated_features(df, threshold=0.9):\n",
    "    try:\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            return \"Input must be a pandas DataFrame.\"\n",
    "        if not (0 <= threshold <= 1):\n",
    "            return \"Threshold must be between 0 and 1.\"\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "        df_reduced = df.drop(columns=to_drop)\n",
    "        return df_reduced\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Mutual Information & Variance Thresholds:\n",
    "\n",
    "**Task 2**: Mutual Information\n",
    "- Step 1: Compute mutual information between features and target.\n",
    "- Step 2: Retain features with high mutual information scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def select_features_by_mutual_info(X, y, threshold=0.1):\n",
    "    try:\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            return \"X must be a pandas DataFrame.\"\n",
    "        if len(X) != len(y):\n",
    "            return \"Feature matrix and target vector length mismatch.\"\n",
    "        mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=0)\n",
    "        selected_features = X.columns[mi_scores >= threshold].tolist()\n",
    "        return selected_features\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Variance Threshold\n",
    "- Step 1: Implement VarianceThreshold to remove features with low variance.\n",
    "- Step 2: Analyze impact on feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def remove_low_variance_features(X, threshold=0.0):\n",
    "    try:\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            return \"X must be a pandas DataFrame.\"\n",
    "        selector = VarianceThreshold(threshold=threshold)\n",
    "        X_selected = selector.fit_transform(X)\n",
    "        selected_columns = X.columns[selector.get_support()].tolist()\n",
    "        return pd.DataFrame(X_selected, columns=selected_columns)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
