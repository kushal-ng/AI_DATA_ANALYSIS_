{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def preprocess_train_infer(train_df, infer_df):\n",
    "    try:\n",
    "        if not isinstance(train_df, pd.DataFrame) or not isinstance(infer_df, pd.DataFrame):\n",
    "            return \"Both inputs must be pandas DataFrames.\"\n",
    "        if train_df.shape[1] != infer_df.shape[1]:\n",
    "            return \"Train and inference data must have the same number of features.\"\n",
    "\n",
    "        pipeline = create_preprocessing_pipeline()\n",
    "        train_processed = pd.DataFrame(pipeline.fit_transform(train_df), columns=train_df.columns)\n",
    "        infer_processed = pd.DataFrame(pipeline.transform(infer_df), columns=infer_df.columns)\n",
    "        return train_processed, infer_processed\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def build_model_pipeline():\n",
    "    try:\n",
    "        pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('classifier', LogisticRegression())\n",
    "        ])\n",
    "        return pipeline\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def train_and_infer(train_df, train_labels, infer_df):\n",
    "    try:\n",
    "        if not (isinstance(train_df, pd.DataFrame) and isinstance(infer_df, pd.DataFrame)):\n",
    "            return \"Inputs must be pandas DataFrames.\"\n",
    "        if train_df.shape[1] != infer_df.shape[1]:\n",
    "            return \"Train and inference data must have the same number of features.\"\n",
    "        if len(train_df) != len(train_labels):\n",
    "            return \"Number of training samples and labels must match.\"\n",
    "\n",
    "        pipeline = build_model_pipeline()\n",
    "        pipeline.fit(train_df, train_labels)\n",
    "        predictions = pipeline.predict(infer_df)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "def build_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "def train_save_pipeline(train_df, train_labels, filepath):\n",
    "    try:\n",
    "        pipeline = build_pipeline()\n",
    "        pipeline.fit(train_df, train_labels)\n",
    "        joblib.dump(pipeline, filepath)\n",
    "        return f\"Pipeline saved to {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error during training/saving: {str(e)}\"\n",
    "\n",
    "def load_and_predict(infer_df, filepath):\n",
    "    try:\n",
    "        pipeline = joblib.load(filepath)\n",
    "        if not isinstance(infer_df, pd.DataFrame):\n",
    "            return \"Inference data must be a pandas DataFrame.\"\n",
    "        predictions = pipeline.predict(infer_df)\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        return f\"Error during loading/prediction: {str(e)}\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
