{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing ML Model Monitoring Pipelines\n",
    "\n",
    "### Model Performance Drift:\n",
    "**Description**: Setup a monitoring pipeline to track key performance metrics (e.g., accuracy, precision) of an ML model over time using a monitoring tool or dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "class ModelPerformanceMonitor:\n",
    "    def __init__(self, log_path='model_performance_log.json'):\n",
    "        self.log_path = log_path\n",
    "        # Load existing log or create new\n",
    "        if os.path.exists(log_path):\n",
    "            with open(log_path, 'r') as f:\n",
    "                self.log = json.load(f)\n",
    "        else:\n",
    "            self.log = []\n",
    "\n",
    "    def log_performance(self, y_true, y_pred, model_version='v1'):\n",
    "        try:\n",
    "            acc = accuracy_score(y_true, y_pred)\n",
    "            prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "            rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                'timestamp': timestamp,\n",
    "                'model_version': model_version,\n",
    "                'accuracy': acc,\n",
    "                'precision': prec,\n",
    "                'recall': rec\n",
    "            }\n",
    "\n",
    "            self.log.append(record)\n",
    "            with open(self.log_path, 'w') as f:\n",
    "                json.dump(self.log, f, indent=2)\n",
    "\n",
    "            print(f\"Logged performance at {timestamp}\")\n",
    "            return record\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging performance: {e}\")\n",
    "            return None\n",
    "\n",
    "    def plot_performance(self):\n",
    "        try:\n",
    "            df = pd.DataFrame(self.log)\n",
    "            if df.empty:\n",
    "                print(\"No logged performance data to plot.\")\n",
    "                return\n",
    "\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "            df = df.sort_values('timestamp')\n",
    "\n",
    "            plt.figure(figsize=(10,6))\n",
    "            plt.plot(df['timestamp'], df['accuracy'], label='Accuracy')\n",
    "            plt.plot(df['timestamp'], df['precision'], label='Precision')\n",
    "            plt.plot(df['timestamp'], df['recall'], label='Recall')\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel('Metric')\n",
    "            plt.title('Model Performance Over Time')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting performance: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    monitor = ModelPerformanceMonitor()\n",
    "\n",
    "    # Simulated true labels and predictions\n",
    "    y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]\n",
    "    y_pred = [1, 0, 0, 1, 0, 1, 1, 0, 1, 0]\n",
    "\n",
    "    monitor.log_performance(y_true, y_pred, model_version='v1')\n",
    "\n",
    "    # Call this regularly or schedule to see performance drift over time\n",
    "    monitor.plot_performance()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Distribution Drift:\n",
    "**Description**: Monitor the distribution of your input features in deployed models to detect any significant shifts from training data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FeatureDistributionMonitor:\n",
    "    def __init__(self, training_data: pd.DataFrame, feature_columns: list):\n",
    "        try:\n",
    "            self.training_data = training_data[feature_columns]\n",
    "            self.feature_columns = feature_columns\n",
    "            self.baseline_distributions = {}\n",
    "            for col in feature_columns:\n",
    "                self.baseline_distributions[col] = self.training_data[col].dropna().values\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing monitor: {e}\")\n",
    "\n",
    "    def detect_drift(self, new_data: pd.DataFrame, alpha=0.05):\n",
    "        try:\n",
    "            results = {}\n",
    "            for col in self.feature_columns:\n",
    "                if col not in new_data.columns:\n",
    "                    results[col] = {'drift': True, 'reason': 'Column missing in new data'}\n",
    "                    continue\n",
    "                \n",
    "                new_values = new_data[col].dropna().values\n",
    "                if len(new_values) == 0 or len(self.baseline_distributions[col]) == 0:\n",
    "                    results[col] = {'drift': False, 'p_value': None, 'reason': 'Insufficient data'}\n",
    "                    continue\n",
    "\n",
    "                stat, p_value = ks_2samp(self.baseline_distributions[col], new_values)\n",
    "                drift = p_value < alpha\n",
    "                results[col] = {'drift': drift, 'p_value': p_value}\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting drift: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def plot_feature_distribution(self, new_data: pd.DataFrame, feature):\n",
    "        try:\n",
    "            if feature not in self.feature_columns:\n",
    "                print(f\"Feature '{feature}' not in monitored features.\")\n",
    "                return\n",
    "            \n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.hist(self.baseline_distributions[feature], bins=30, alpha=0.5, label='Training')\n",
    "            if feature in new_data.columns:\n",
    "                plt.hist(new_data[feature].dropna(), bins=30, alpha=0.5, label='New Data')\n",
    "            plt.title(f'Distribution Comparison for {feature}')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error plotting distributions: {e}\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate training data\n",
    "    train_df = pd.DataFrame({\n",
    "        'feature1': np.random.normal(0, 1, 1000),\n",
    "        'feature2': np.random.uniform(0, 10, 1000)\n",
    "    })\n",
    "\n",
    "    monitor = FeatureDistributionMonitor(train_df, ['feature1', 'feature2'])\n",
    "\n",
    "    # Simulate new incoming data (with drift in feature1)\n",
    "    new_df = pd.DataFrame({\n",
    "        'feature1': np.random.normal(1, 1, 1000),  # shifted mean\n",
    "        'feature2': np.random.uniform(0, 10, 1000)\n",
    "    })\n",
    "\n",
    "    drift_results = monitor.detect_drift(new_df)\n",
    "    print(drift_results)\n",
    "\n",
    "    # Plot distributions for feature1\n",
    "    monitor.plot_feature_distribution(new_df, 'feature1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection in Predictions:\n",
    "**DEscription**: Implement an anomaly detection mechanism to flag unusual model\n",
    "predictions. Simulate anomalies by altering input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "class PredictionAnomalyDetector:\n",
    "    def __init__(self, contamination=0.01):\n",
    "        try:\n",
    "            self.model = IsolationForest(contamination=contamination, random_state=42)\n",
    "            self.fitted = False\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing detector: {e}\")\n",
    "\n",
    "    def fit(self, predictions: pd.Series):\n",
    "        try:\n",
    "            X = predictions.values.reshape(-1, 1)\n",
    "            self.model.fit(X)\n",
    "            self.fitted = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error fitting model: {e}\")\n",
    "\n",
    "    def predict(self, new_predictions: pd.Series):\n",
    "        try:\n",
    "            if not self.fitted:\n",
    "                raise ValueError(\"Model not fitted yet. Call fit() first.\")\n",
    "            X_new = new_predictions.values.reshape(-1, 1)\n",
    "            anomaly_labels = self.model.predict(X_new)  # -1 for anomaly, 1 for normal\n",
    "            anomalies = new_predictions[anomaly_labels == -1]\n",
    "            return anomalies\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting anomalies: {e}\")\n",
    "            return pd.Series(dtype=float)\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulate normal predictions\n",
    "    preds = pd.Series(np.random.normal(loc=50, scale=5, size=1000))\n",
    "\n",
    "    # Insert anomalies by adding extreme values\n",
    "    preds_with_anomalies = preds.copy()\n",
    "    anomalies_indices = np.random.choice(preds.index, size=10, replace=False)\n",
    "    preds_with_anomalies.loc[anomalies_indices] = preds_with_anomalies.loc[anomalies_indices] + np.random.uniform(20, 50, size=10)\n",
    "\n",
    "    detector = PredictionAnomalyDetector(contamination=0.01)\n",
    "    detector.fit(preds)\n",
    "    anomalies = detector.predict(preds_with_anomalies)\n",
    "    \n",
    "    print(\"Anomalies detected at indices:\")\n",
    "    print(anomalies.index.tolist())\n",
    "    print(anomalies.values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
