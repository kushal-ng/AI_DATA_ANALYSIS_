{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "import great_expectations as ge\n",
    "\n",
    "# 1. Pandas-Profiling\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Generate full profile report with correlations\n",
    "profile_full = ProfileReport(df, title=\"Full Data Profile with Correlations\", correlations={\"pearson\": {\"calculate\": True}})\n",
    "profile_full.to_file(\"full_profile_report.html\")\n",
    "\n",
    "# Generate profile report for a subset of columns\n",
    "subset_cols = ['column1', 'column2', 'column3']  # replace with your actual column names\n",
    "profile_subset = ProfileReport(df[subset_cols], title=\"Subset Data Profile\")\n",
    "profile_subset.to_file(\"subset_profile_report.html\")\n",
    "\n",
    "\n",
    "# 2. Great Expectations\n",
    "\n",
    "# Initialize a GE Data Context and create a dataset\n",
    "gdf = ge.from_pandas(df)\n",
    "\n",
    "# Create a new expectation suite\n",
    "suite_name = \"basic_expectations\"\n",
    "suite = gdf.get_expectation_suite(suite_name, overwrite_existing=True)\n",
    "\n",
    "# Add expectations\n",
    "gdf.expect_column_values_to_not_be_null('column1')\n",
    "gdf.expect_column_values_to_be_in_type_list('column2', ['int64', 'float64'])\n",
    "gdf.expect_column_mean_to_be_between('column3', min_value=0, max_value=100)\n",
    "\n",
    "# Save expectation suite\n",
    "gdf.save_expectation_suite(suite_name)\n",
    "\n",
    "# Validate data against the expectation suite\n",
    "results = gdf.validate(expectation_suite=suite_name)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import great_expectations as ge\n",
    "import logging\n",
    "import smtplib\n",
    "from email.message import EmailMessage\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def send_email_alert(subject, body, to_emails):\n",
    "    try:\n",
    "        msg = EmailMessage()\n",
    "        msg.set_content(body)\n",
    "        msg['Subject'] = subject\n",
    "        msg['From'] = 'your_email@example.com'  # replace with sender email\n",
    "        msg['To'] = ', '.join(to_emails)\n",
    "\n",
    "        # Configure SMTP server (example for Gmail)\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:\n",
    "            smtp.login('your_email@example.com', 'your_email_password')  # replace with credentials\n",
    "            smtp.send_message(msg)\n",
    "        logging.info(\"Email alert sent successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to send email alert: {e}\")\n",
    "\n",
    "def monitor_data_quality(df, expectation_suite_name):\n",
    "    gdf = ge.from_pandas(df)\n",
    "    results = gdf.validate(expectation_suite=expectation_suite_name)\n",
    "    if not results[\"success\"]:\n",
    "        logging.warning(\"Data quality dropped: Expectation(s) failed.\")\n",
    "        # Prepare email alert content\n",
    "        failed_expectations = [r for r in results['results'] if not r['success']]\n",
    "        body = f\"Data quality alert! {len(failed_expectations)} expectations failed.\\nDetails:\\n\"\n",
    "        for exp in failed_expectations:\n",
    "            body += f\"- Expectation: {exp['expectation_config']['expectation_type']}\\n\"\n",
    "            body += f\"  Result: {exp['result']}\\n\"\n",
    "        send_email_alert(\"Data Quality Alert\", body, [\"recipient@example.com\"])  # replace with actual recipient(s)\n",
    "    else:\n",
    "        logging.info(\"Data quality check passed successfully.\")\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    import pandas as pd\n",
    "    # Load or create your dataframe here\n",
    "    df = pd.read_csv('data.csv')  # Replace with your data source\n",
    "    expectation_suite_name = 'basic_expectations'  # Ensure this exists in GE context\n",
    "    \n",
    "    monitor_data_quality(df, expectation_suite_name)\n",
    "\n",
    "# Notes:\n",
    "# - For Grafana: set up a Prometheus data source or push metrics to a time series DB\n",
    "# - Create alert rules in Grafana dashboard based on metrics from your monitoring system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def train_isolation_forest(data, contamination=0.1, random_state=42):\n",
    "    try:\n",
    "        model = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "        model.fit(data)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error training Isolation Forest: {e}\")\n",
    "        return None\n",
    "\n",
    "def detect_anomalies(model, data):\n",
    "    try:\n",
    "        preds = model.predict(data)\n",
    "        # Isolation Forest outputs: -1 for anomalies, 1 for normal points\n",
    "        anomalies = data[preds == -1]\n",
    "        return anomalies\n",
    "    except Exception as e:\n",
    "        print(f\"Error detecting anomalies: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def custom_outlier_detection(data, threshold=3):\n",
    "    try:\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0)\n",
    "        z_scores = (data - mean) / std\n",
    "        # Identify outliers where absolute z-score > threshold\n",
    "        outliers_mask = (np.abs(z_scores) > threshold).any(axis=1)\n",
    "        return data[outliers_mask]\n",
    "    except Exception as e:\n",
    "        print(f\"Error in custom outlier detection: {e}\")\n",
    "        return np.array([])\n",
    "\n",
    "def monitoring_function(data, model=None, use_ai=True):\n",
    "    if use_ai:\n",
    "        if model is None:\n",
    "            print(\"No pre-trained model provided.\")\n",
    "            return None\n",
    "        anomalies = detect_anomalies(model, data)\n",
    "        if not anomalies.empty:\n",
    "            print(f\"Detected {len(anomalies)} anomalies using AI model.\")\n",
    "        else:\n",
    "            print(\"No anomalies detected by AI model.\")\n",
    "        return anomalies\n",
    "    else:\n",
    "        outliers = custom_outlier_detection(data)\n",
    "        if outliers.size > 0:\n",
    "            print(f\"Detected {len(outliers)} outliers using custom logic.\")\n",
    "        else:\n",
    "            print(\"No outliers detected by custom logic.\")\n",
    "        return outliers\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample data: columns could be e.g. age and income\n",
    "    sample_data = np.array([[25, 50000], [30, 60000], [35, 75000], [40, 100000], [120, 500], [28, 62000]])\n",
    "    df = pd.DataFrame(sample_data, columns=['age', 'income'])\n",
    "    \n",
    "    # Train model\n",
    "    isolation_forest_model = train_isolation_forest(df)\n",
    "    \n",
    "    # Monitor data with AI\n",
    "    anomalies = monitoring_function(df, model=isolation_forest_model, use_ai=True)\n",
    "    \n",
    "    # Monitor data with custom logic\n",
    "    outliers = monitoring_function(sample_data, use_ai=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
