{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Understanding and Defining Data Quality Metrics\n",
    "**Description**: Learn how to define basic data quality metrics such as completeness, validity, and uniqueness for a simple dataset.\n",
    "\n",
    "**Steps**:\n",
    "1. Dataset: Use a CSV with columns like Name , Email , Age .\n",
    "2. Metric Definitions:\n",
    "    - Completeness: Percentage of non-null values.\n",
    "    - Validity: % of email fields containing @ .\n",
    "    - Uniqueness: Count distinct entries in the Email column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_completeness(df):\n",
    "    completeness = df.notnull().mean() * 100\n",
    "    return completeness.to_dict()\n",
    "\n",
    "def calculate_validity_email(df, email_col='Email'):\n",
    "    if email_col not in df.columns:\n",
    "        return None\n",
    "    valid_emails = df[email_col].dropna().apply(lambda x: '@' in str(x))\n",
    "    validity = (valid_emails.sum() / len(df)) * 100\n",
    "    return validity\n",
    "\n",
    "def calculate_uniqueness(df, col):\n",
    "    if col not in df.columns:\n",
    "        return None\n",
    "    unique_count = df[col].nunique()\n",
    "    return unique_count\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    \n",
    "    completeness = calculate_completeness(df)\n",
    "    validity = calculate_validity_email(df)\n",
    "    uniqueness = calculate_uniqueness(df, 'Email')\n",
    "    \n",
    "    print(f\"Completeness (% non-null per column): {completeness}\")\n",
    "    print(f\"Validity (% emails containing '@'): {validity:.2f}%\")\n",
    "    print(f\"Uniqueness (distinct emails count): {uniqueness}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Calculating Data Quality Score\n",
    "**Description**: Aggregate multiple metrics to calculate an overall data quality score.\n",
    "\n",
    "**Steps**:\n",
    "1. Formula: Simple average of all metrics defined in Task 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_completeness(df):\n",
    "    completeness = df.notnull().mean() * 100\n",
    "    return completeness.mean()\n",
    "\n",
    "def calculate_validity_email(df, email_col='Email'):\n",
    "    if email_col not in df.columns:\n",
    "        return 0.0\n",
    "    valid_emails = df[email_col].dropna().apply(lambda x: '@' in str(x))\n",
    "    validity = (valid_emails.sum() / len(df)) * 100\n",
    "    return validity\n",
    "\n",
    "def calculate_uniqueness(df, col):\n",
    "    if col not in df.columns:\n",
    "        return 0.0\n",
    "    unique_count = df[col].nunique()\n",
    "    total_count = len(df)\n",
    "    uniqueness = (unique_count / total_count) * 100 if total_count > 0 else 0.0\n",
    "    return uniqueness\n",
    "\n",
    "def calculate_overall_dqi(df):\n",
    "    completeness_score = calculate_completeness(df)\n",
    "    validity_score = calculate_validity_email(df)\n",
    "    uniqueness_score = calculate_uniqueness(df, 'Email')\n",
    "    scores = [completeness_score, validity_score, uniqueness_score]\n",
    "    overall_score = sum(scores) / len(scores)\n",
    "    return overall_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    overall_dqi = calculate_overall_dqi(df)\n",
    "    print(f\"Overall Data Quality Score: {overall_dqi:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Creating Expectations for a CSV\n",
    "**Description**: Develop basic data quality expectations using Great Expectations.\n",
    "\n",
    "**Steps**:\n",
    "1. Expectation Suite\n",
    "2. Define Expectations for Completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import great_expectations as ge\n",
    "\n",
    "# Load CSV as a Great Expectations dataset\n",
    "df = ge.read_csv(\"data.csv\")\n",
    "\n",
    "# Create an expectation suite\n",
    "suite_name = \"basic_completeness_suite\"\n",
    "suite = df.get_expectation_suite(suite_name)\n",
    "\n",
    "# Define expectations for completeness on specific columns\n",
    "columns_to_check = ['Name', 'Email', 'Age']\n",
    "for col in columns_to_check:\n",
    "    df.expect_column_values_to_not_be_null(column=col)\n",
    "\n",
    "# Save the expectation suite (optional)\n",
    "df.save_expectation_suite(suite_name=suite_name, discard_failed_expectations=False)\n",
    "\n",
    "# Validate the dataset against the suite\n",
    "results = df.validate(expectation_suite=suite_name)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Running and Validating Expectations\n",
    "**Description**: Run the created expectations and generate an output report.\n",
    "\n",
    "**Steps**:\n",
    "1. Validate\n",
    "2. Generate HTML Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import great_expectations as ge\n",
    "from great_expectations.render.renderer import ExpectationSuiteHTMLRenderer\n",
    "from great_expectations.render.view import DefaultJinjaPageView\n",
    "from great_expectations.data_context import DataContext\n",
    "\n",
    "# Load CSV as a GE dataset\n",
    "df = ge.read_csv(\"data.csv\")\n",
    "\n",
    "# Validate the dataset using the existing expectation suite\n",
    "suite_name = \"basic_completeness_suite\"\n",
    "results = df.validate(expectation_suite=suite_name)\n",
    "\n",
    "# Generate an HTML report for the validation results\n",
    "renderer = ExpectationSuiteHTMLRenderer()\n",
    "rendered_content = renderer.render(validation_result_suite=results)\n",
    "\n",
    "view = DefaultJinjaPageView(rendered_content)\n",
    "html_report = view.render()\n",
    "\n",
    "# Save the report as an HTML file\n",
    "with open(\"validation_report.html\", \"w\") as f:\n",
    "    f.write(html_report)\n",
    "\n",
    "print(\"Validation complete. Report saved as validation_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Automating Data Quality Score Calculation\n",
    "**Description**: Automate the data quality score via a script that integrates with Great\n",
    "Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import great_expectations as ge\n",
    "\n",
    "def calculate_data_quality_score(csv_file, suite_name):\n",
    "    df = ge.read_csv(csv_file)\n",
    "    results = df.validate(expectation_suite=suite_name)\n",
    "    \n",
    "    # Extract successful expectations count and total expectations count\n",
    "    successful = sum([res['success'] for res in results['results']])\n",
    "    total = len(results['results'])\n",
    "    \n",
    "    # Calculate data quality score as percentage of successful expectations\n",
    "    dqi_score = (successful / total) * 100 if total > 0 else 0.0\n",
    "    return dqi_score\n",
    "\n",
    "# Example usage\n",
    "csv_path = \"data.csv\"\n",
    "expectation_suite = \"basic_completeness_suite\"\n",
    "\n",
    "score = calculate_data_quality_score(csv_path, expectation_suite)\n",
    "print(f\"Automated Data Quality Score: {score:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning\n",
    "**Description**: Implement a system where if data quality metrics fall below a threshold,\n",
    "automated data cleaning scripts are triggered.\n",
    "\n",
    "**Steps**:\n",
    "1. Define Cleaning Logic\n",
    "2. Integrate with Great Expectations:\n",
    "    - Use an action within the Great Expectations action list that only triggers if quality score is below a threshold, automating the cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here\n",
    "import great_expectations as ge\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(df):\n",
    "    # Example cleaning logic: drop rows with any null values\n",
    "    return df.dropna()\n",
    "\n",
    "def calculate_data_quality_score(results):\n",
    "    successful = sum([res['success'] for res in results['results']])\n",
    "    total = len(results['results'])\n",
    "    return (successful / total) * 100 if total > 0 else 0.0\n",
    "\n",
    "def automate_cleaning_if_needed(csv_file, suite_name, threshold=90):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    ge_df = ge.from_pandas(df)\n",
    "    \n",
    "    validation_results = ge_df.validate(expectation_suite=suite_name)\n",
    "    dqi_score = calculate_data_quality_score(validation_results)\n",
    "    \n",
    "    print(f\"Current Data Quality Score: {dqi_score:.2f}%\")\n",
    "    if dqi_score < threshold:\n",
    "        print(\"Data quality below threshold. Triggering automated cleaning...\")\n",
    "        cleaned_df = clean_data(df)\n",
    "        cleaned_df.to_csv(csv_file, index=False)\n",
    "        print(\"Data cleaning completed and saved.\")\n",
    "    else:\n",
    "        print(\"Data quality above threshold. No cleaning needed.\")\n",
    "\n",
    "# Example usage\n",
    "csv_path = \"data.csv\"\n",
    "expectation_suite = \"basic_completeness_suite\"\n",
    "quality_threshold = 90\n",
    "\n",
    "automate_cleaning_if_needed(csv_path, expectation_suite, quality_threshold)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
